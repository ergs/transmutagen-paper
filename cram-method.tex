Consider the function $e^{-t}$ on the interval $[0,\infty)$. Let
$\pi_{k,k}\subset \mathbb{R}(t)$ be the set of rational functions $r_{k,k}(t)
= p_k(t)/q_k(t)$, where $p_k(t), q_k(t)\in \mathbb{R}[t]$ are polynomials of
degree at most $k$.

Chebyshev Rational Approximation Method (CRAM) approximation of degree $k$
for $e^{-t}$ is the unique rational function $\hat{r}_{k,k}(t)$ such that
\begin{equation}
  \sup_{t\in[0, \infty)}|\hat{r}_{k, k}(t) - e^{-t}|
\end{equation}
is minimized. That is,
\begin{equation}
  \epsilon_{k,k} = \sup_{t\in[0, \infty)}|\hat{r}_{k, k}(t) - e^{-t}| = \inf_{r_{k,k}\in\pi_{k,k}}\left\{\sup_{t\in[0, \infty)}|r_{k, k}(t) - e^{-t}|\right\}.
\end{equation}
$\epsilon_{k,k}$ is the absolute error of the approximation. It has been shown
that $\epsilon_{k,k} = O(H^{-k})$, where $H=9.289\ldots$.

Transmutagen implements the Remez algorithm to compute the CRAM approximation.
All computations are done using SymPy symbolic expressions with arbitrary
precision floating point numbers, which are backed by the mpmath library.

The interval $[0, \infty)$ is first translated to $[-1, 1)$ via the
transformation $t\mapsto c\frac{t+1}{t-1}$.
% cite Carpenter paper here
% TODO: discuss this *after* discussing finding the roots.
The constant $c$ is chosen so that the minimax roots of the approximation are
distributed sufficiently evenly across the interval $[-1, 1)$. We found that
the value $c=0.6k$ works well.

The expressions
\begin{equation}
  r := \frac{p_kt^k + \cdots + p_1t + p_0}{q_kt^k + \cdots +
    q_1t + 1},
\end{equation}
\begin{equation}
  D := e^{c\frac{t+1}{t-1}} - r,
\end{equation}
and
\begin{equation}
  E := D + (-1)^i\epsilon
\end{equation}
are represented symbolically as a SymPy expression. This expression $E$ is then
evaluated in $t$ at $2(k + 1)$ points in the interval $(-1, 1)$ for $i=0\ldots
2k+1$. The Chebyshev nodes were used for the initial points, that
is, the roots of $T_{2(k +1)}(x)$, which always lie in the interval $(-1, 1)$
($T_n(x)$ is defined as $T_n(\cos(x)) = \cos(nx)$). The Chebyshev nodes are
used as an initial guess, but any set of points on the interval $(-1, 1)$
work. We found that convergence can take more than twice as many steps when
random initial points are used instead of the Chebyshev nodes. % TODO: verify this
This results in a nonlinear system of $2(k+1)$ equations in $2(k+1)$ variables
($p_0,\ldots,p_k,q_1,\ldots,q_k,\epsilon$). These are solved using SymPy's
\texttt{nsolve} function, which internally uses \texttt{mpmath.findroot}. This
uses Newton's method, with a symbolically computed Jacobian.

The solution to this system is then substituted into $D$. The critical points
of this function on the interval $[-1, 1)$ are then found. This is done by
taking the symbolic derivative of $D$, splitting the interval into
subintervals, and performing bisection using \texttt{nsolve} on each
subinterval. Call these points, along with the end-points $D|_{t=-1}$ and
$\lim_{t\to 1} D=-r|_{t=1}$, $z_i$. There are $2(k+1)$ points $z_i$. The
points $z_i$ are used as the set of initial points and the algorithm iterates
as such until convergence is reached.

By the theory, the approximation $\hat{r}_{k, k}$ is known to be minimal for a
given order $k$ when the critical points of $\hat{r}_{k, k}(t) - e^{-t}$
osculate in sign and have equal absolute value. For the iterates of the
algorithm, the points $z_i$ are these critical points of the current
approximation, so convergence is detected by looking at
$\varepsilon_N = \max{|z_i|} - \min{|z_i|}$, for the $N$th iteration of the
algorithm. Convergence occurs roughly when $\varepsilon_N$ is near $10^{-d}$,
where $d$ decimal digits are used in the calculations. However, the true
minimal value of $\varepsilon_N$ depends on both $k$ and $d$. We found a
robust heuristic to be to iterate until $\log_{10}{(\varepsilon_N)}$ is near $-d$,
then stop iterating when the values of $\varepsilon_N$ become log-convex.
% TODO: Add convergence plot here.
