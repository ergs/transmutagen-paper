Consider the function $e^{-t}$ on the interval $[0,\infty)$. Let
$\pi_{k,k}\subset \mathbb{R}(t)$ be the set of rational functions $r_{k,k}(t)
= p_k(t)/q_k(t)$, where $p_k(t), q_k(t)\in \mathbb{R}[t]$ are polynomials of
degree at most $k$.

Chebyshev Rational Approximation Method (CRAM)~\cite{ationneeded} approximation of degree $k$
for $e^{-t}$ is the unique rational function $\hat{r}_{k,k}(t)$ such that
\begin{equation}
  \sup_{t\in[0, \infty)}|\hat{r}_{k, k}(t) - e^{-t}|
\end{equation}
is minimized. That is,
\begin{equation}
  \epsilon_{k,k} = \sup_{t\in[0, \infty)}|\hat{r}_{k, k}(t) - e^{-t}| = \inf_{r_{k,k}\in\pi_{k,k}}\left\{\sup_{t\in[0, \infty)}|r_{k, k}(t) - e^{-t}|\right\}.
\end{equation}
$\epsilon_{k,k}$ is the absolute error of the approximation. It has been shown
that $\epsilon_{k,k} = O(H^{-k})$, where $H=9.289\ldots$~\cite{ationneeded}.

Transmutagen implements the Remez algorithm to compute the CRAM approximation.
All computations are done using SymPy~\cite{10.7717/peerj-cs.103} symbolic expressions with arbitrary
precision floating point numbers, which are backed by the mpmath library~\cite{ationneeded}.

The interval $[0, \infty)$ is first translated to $[-1, 1)$ via the
transformation $t\mapsto c\frac{t+1}{t-1}$.
% cite Carpenter paper here
% TODO: discuss this *after* discussing finding the roots.
The constant $c$ is chosen so that the minimax roots of the approximation are
distributed sufficiently evenly across the interval $[-1, 1)$. We found that
the value $c=0.6k$ works well.

The expressions
\begin{equation}
  r := \frac{p_kt^k + \cdots + p_1t + p_0}{q_kt^k + \cdots +
    q_1t + 1},
\end{equation}
\begin{equation}
  D := e^{c\frac{t+1}{t-1}} - r,
\end{equation}
and
\begin{equation}
  E := D + (-1)^i\epsilon
\end{equation}
are represented symbolically as a SymPy expression. This expression $E$ is
then evaluated in $t$ at $2(k + 1)$ points in the interval $(-1, 1)$ for
$i=0\ldots 2k+1$. For the first iteration of the algorithm, any set of initial
points can be used. We used the Chebyshev nodes~\cite{ationneeded}, that is,
the roots of $T_{2(k +1)}(x)$, which always lie in the interval $(-1, 1)$
($T_n(x)$ is defined as $T_n(\cos(x)) = \cos(nx)$). We found that convergence
can take more than twice as many steps when random initial points are used
instead of the Chebyshev nodes. % TODO: verify this
This results in a nonlinear system of $2(k+1)$ equations in $2(k+1)$ variables
($p_0,\ldots,p_k,q_1,\ldots,q_k,\epsilon$). These are solved using SymPy's
\texttt{nsolve} function, which internally uses \texttt{mpmath.findroot}. This
uses Newton's method~\cite{ationneeded}, with a symbolically computed
Jacobian.

The solution to this system is then substituted into $D$. The critical points
of this function on the interval $[-1, 1)$ are then found. This is done by
taking the symbolic derivative of $D$, splitting the interval into
subintervals, and performing bisection using \texttt{nsolve} on each
subinterval. Call these points, along with the end-points $D|_{t=-1}$ and
$\lim_{t\to 1} D=-r|_{t=1}$, $\{z_i\}$. There are $2(k+1)$ points $\{z_i\}$. The
points $\{z_i\}$ are used as the set of initial points, and the algorithm iterates
as such until convergence is reached.

By the theory, the approximation $\hat{r}_{k, k}$ is known to be minimal for a
given order $k$ when the critical points of $\hat{r}_{k, k}(t) - e^{-t}$
osculate in sign and have equal absolute value. For the iterates of the
algorithm, the points $z_i$ are these critical points of the current
approximation, so convergence is detected by looking at
$\varepsilon_N = \max{|z_i|} - \min{|z_i|}$, for the $N$th iteration of the
algorithm. Convergence occurs roughly when $\varepsilon_N$ is near $10^{-d}$,
where $d$ decimal digits are used in the calculations. However, the true
minimal value of $\varepsilon_N$ depends on both $k$ and $d$. We found a
robust heuristic to be to iterate until $\log_{10}{(\varepsilon_N)}$ is near $-d$,
then stop iterating when the values of $\varepsilon_N$ become log-convex.
% TODO: Add convergence plot here.

Once the algorithm converges, the rational function is translated back to the
interval $[0, \infty)$ via the inverse transformation $t\mapsto \frac{t - c}{t
  + c}$. This must be done with care to avoid losing precision. For a
polynomial $f$ and M\"{o}bius transformation~\cite{ationneeded} $p/q$, the SymPy
method \texttt{Poly.transform} efficiently computes
$q^nf\left(p/q\right)$ without losing precision. The resulting
rational function is normalized so that the constant term in the denominator
is 1, which matches the form used in the literature. % TODO: Citation(s)

\subsection{Applying CRAM to matrices}
Given a square matrix $A$, the matrix exponential is defined
as~\cite{ationneeded}
\begin{equation}
  e^{A} = \sum_{n=0}^\infty \frac{A^n}{n!}.
\end{equation}

The CRAM approximation to $e^{-x}$ can be applied to approximate the matrix
exponential when the matrix $A$ has eigenvalues on or near the negative real
axis~\cite{pusa2010computing}. This applies to transmutation and decay
matrices.
% TODO: Show plots of eigenvalues of transmutation and decay matrices
% TODO: Show plot of accuracy of CRAM in complex plane.

% SymPy to compute Halphen's constant
% nsolve([1/s - exp(-pi*elliptic_k(1 - c**2)/elliptic_k(c**2)), elliptic_k(c**2) - 2*elliptic_e(c**2)], [c, s], [.9, 9])

\subsection{Results}

Transmutagen is able to compute the CRAM approximation for any degree $n$ to
any number of decimal digits $d$. We have computed the first $60$ degrees to
200 digits. Our digits agree with the coefficients published
in~\cite{carpenter1984extended}, which has the first 30 degrees up to 20
digits. It should be noted that the digits in~\cite{carpenter1984extended}
have been rounded using decimal rounding, which is often different from
floating point, or binary rounding. For example, the constant coefficient in
the numerator ($p_0$) for degree 11 is reported as
\texttt{1.0000000000146631119}, even though the true coefficient,
\texttt{1.000000000014663111949374871...} is more closely approximated as a
binary floating point number by \texttt{1.0000000000146631120}. Our computed
digits agree with those from~\cite{carpenter1984extended} when rounded using
the default half-even strategy of the Python \texttt{decimal} module. This is
important because for practical purposes, when using the computed coefficients
to compute a CRAM approximation on a computer, one should round the digits to
a machine floating point number using binary rounding, to get the floating
point number that most closely approximates the true value. The difference is
typically no more than a single bit, but such errors in the approximation can
propagate to larger errors in the result, as shown in section ???.
%TODO: Write something about that in another section
