Consider the function $e^{-t}$ on the interval $[0,\infty)$. Let
$\pi_{k,k}\subset \mathbb{R}(t)$ be the set of rational functions $r_{k,k}(t)
= p_k(t)/q_k(t)$, where $p_k(t), q_k(t)\in \mathbb{R}[t]$ are polynomials of
degree at most $k$.

Chebyshev Rational Approximation Method (CRAM) approximation of degree $k$
for $e^{-t}$ is the unique rational function $\hat{r}_{k,k}(t)$ such that
\begin{equation}
  \sup_{t\in[0, \infty)}|\hat{r}_{k, k}(t) - e^{-t}|
\end{equation}
is minimized. That is,
\begin{equation}
  \epsilon_{k,k} = \sup_{t\in[0, \infty)}|\hat{r}_{k, k}(t) - e^{-t}| = \inf_{r_{k,k}\in\pi_{k,k}}\left\{\sup_{t\in[0, \infty)}|r_{k, k}(t) - e^{-t}|\right\}.
\end{equation}
$\epsilon_{k,k}$ is the absolute error of the approximation. It has been shown
that $\epsilon_{k,k} = O(H^{-k})$, where $H=9.289\ldots$.

Transmutagen implements the Remez algorithm to compute the CRAM approximation.
All computations are done using SymPy~\cite{10.7717/peerj-cs.103} symbolic expressions with arbitrary
precision floating point numbers, which are backed by the mpmath library.

The interval $[0, \infty)$ is first translated to $[-1, 1)$ via the
transformation $t\mapsto c\frac{t+1}{t-1}$.
% cite Carpenter paper here
% TODO: discuss this *after* discussing finding the roots.
The constant $c$ is chosen so that the minimax roots of the approximation are
distributed sufficiently evenly across the interval $[-1, 1)$. We found that
the value $c=0.6k$ works well.

The expressions
\begin{equation}
  r := \frac{p_kt^k + \cdots + p_1t + p_0}{q_kt^k + \cdots +
    q_1t + 1},
\end{equation}
\begin{equation}
  D := e^{c\frac{t+1}{t-1}} - r,
\end{equation}
and
\begin{equation}
  E := D + (-1)^i\epsilon
\end{equation}
are represented symbolically as a SymPy expression. This expression $E$ is
then evaluated in $t$ at $2(k + 1)$ points in the interval $(-1, 1)$ for
$i=0\ldots 2k+1$. For the first iteration of the algorithm, any set of initial
points can be used. We used the Chebyshev nodes, that is, the roots of
$T_{2(k +1)}(x)$, which always lie in the interval $(-1, 1)$ ($T_n(x)$ is
defined as $T_n(\cos(x)) = \cos(nx)$). We found that convergence can take more
than twice as many steps when random initial points are used instead of the
Chebyshev nodes. % TODO: verify this
This results in a nonlinear system of $2(k+1)$ equations in $2(k+1)$ variables
($p_0,\ldots,p_k,q_1,\ldots,q_k,\epsilon$). These are solved using SymPy's
\texttt{nsolve} function, which internally uses \texttt{mpmath.findroot}. This
uses Newton's method, with a symbolically computed Jacobian.

The solution to this system is then substituted into $D$. The critical points
of this function on the interval $[-1, 1)$ are then found. This is done by
taking the symbolic derivative of $D$, splitting the interval into
subintervals, and performing bisection using \texttt{nsolve} on each
subinterval. Call these points, along with the end-points $D|_{t=-1}$ and
$\lim_{t\to 1} D=-r|_{t=1}$, $\{z_i\}$. There are $2(k+1)$ points $\{z_i\}$. The
points $\{z_i\}$ are used as the set of initial points, and the algorithm iterates
as such until convergence is reached.

By the theory, the approximation $\hat{r}_{k, k}$ is known to be minimal for a
given order $k$ when the critical points of $\hat{r}_{k, k}(t) - e^{-t}$
osculate in sign and have equal absolute value. For the iterates of the
algorithm, the points $z_i$ are these critical points of the current
approximation, so convergence is detected by looking at
$\varepsilon_N = \max{|z_i|} - \min{|z_i|}$, for the $N$th iteration of the
algorithm. Convergence occurs roughly when $\varepsilon_N$ is near $10^{-d}$,
where $d$ decimal digits are used in the calculations. However, the true
minimal value of $\varepsilon_N$ depends on both $k$ and $d$. We found a
robust heuristic to be to iterate until $\log_{10}{(\varepsilon_N)}$ is near $-d$,
then stop iterating when the values of $\varepsilon_N$ become log-convex.
% TODO: Add convergence plot here.

Once the algorithm converges, the rational function is translated back to the
interval $[0, \infty)$ via the inverse transformation $t\mapsto \frac{t - c}{t
  + c}$. This must be done with care to avoid losing precision. For a
polynomial $f$ and M\"{o}bius transformation $p/q$, the SymPy
method \texttt{Poly.transform} efficiently computes
$q^nf\left(p/q\right)$ without losing precision. The resulting
rational function is normalized so that the constant term in the denominator
is 1, which matches the form used in the literature. % TODO: Citation(s)

\subsection{Applying CRAM to matrices}
Given a square matrix $A$, the matrix exponential is defined as
\begin{equation}
  e^{A} = \sum_{n=0}^\infty \frac{A^n}{n!}.
\end{equation}

The CRAM approximation to $e^{-x}$ can be applied to approximate the matrix
exponential when the matrix $A$ has eigenvalues on or near the negative real
axis~\cite{pusa2010computing}. This applies to transmutation and decay
matrices.
% TODO: Show plots of eigenvalues of transmutation and decay matrices
% TODO: Show plot of accuracy of CRAM in complex plane.

\subsection{Properties of CRAM}

Let $\hat{r}_{k, k}(t)=\frac{p(t)}{q(t)}$ where $p$ and $q$ are polynomials
and $q(0) = 1$. Note that $q(t)$ has no roots of multiplicity greater than 1.

Indeed, suppose $\theta$ were a root $q(t)$ with $q(t) = q'(t)(t - \theta)^2$.
There are two cases. If $\theta$ is real, then $\theta < 0$
(otherwise $\hat{r}_{k, k}(t)$ would have a pole on the real line and could
not be the optimal CRAM approximation to $e^{-t}$). Let $\epsilon > 0$. Let
$r'(t) = \frac{p(t)}{(t + \epsilon)}$ Then,
for some $t' > 0$
\begin{equation}
|
\end{equation}
Let
$T > t$. Then
\begin{equation}
0 < \frac{p(t)}{(t - \theta)^2q'(t)} - e^{-t} < \frac{p(T)}{(T
  - \theta)^2q'(T)} - e^{-T} < \epsilon_{k, k},
\end{equation}
by the equioscillation theorem.

%  can be represented via a partial fraction decomposition:
% \begin{equation}
%   \hat{r}_{k, k}(t) = \alpha_0 + \sum_{i=1}^k \frac{\alpha_i}{t - \theta_i},
% \end{equation}
% where $\theta_i$ are the roots of $q_kt^k + \cdots + q_1t + 1$, which are all
% nonreal and have multiplicity 1, $\alpha_i$ is the residue of
% $\hat{r}_{k, k}(t)$ at $t=\theta_i$, and $\alpha_0$ is the residue at
% infinity. When $k$ is even, the roots $\theta_i$ all come in complex conjugate
% pairs, so the sum can be reduced to
% \begin{equation}
%   \hat{r}_{k, k}(t) = \alpha_0 + \mathrm{Re}\left(\sum_{i=1}^{k/2} \frac{\alpha_i}{t - \theta_i}\right),
% \end{equation}
% which is computed on a matrix $A$ against a vector $b$ via
% \begin{equation}
%   \hat{r}_{k, k}(A)b = \alpha_0b + \mathrm{Re}\left(\sum_{i=1}^{k/2} (A -
%     \theta_i I)\backslash(\alpha_i b) \right).
% \end{equation}
