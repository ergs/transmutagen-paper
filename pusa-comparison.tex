In addition, we have compared our partial fraction digits to those published
in~\cite{pusa2012correction} for degrees 14 and 16. However, many
discrepancies were found. These discrepancies are summarized in
Tables~\ref{table:pusa-degree-14} and~\ref{table:pusa-degree-16}.

Two values, $\mathrm{re}(\alpha_5)$ and $\mathrm{re}(\alpha_7)$ of degree 16,
differed within machine floating point precision (each within one-bit). The
values cause the approximations to differ by as much as $6\times10^{-19}$ (see
Figure~\ref{fig:pusa-differences}).

\setul{}{1pt}

\begin{table}[h!]
\centering
\begin{tabular}{ r b{1.8in} b{1.8in} }
Coefficient & Values from~\cite{pusa2012correction} & Values computed by Transmutagen \\
\midrule
\input{pusa-table-14.tex}
\bottomrule
\end{tabular}
\caption{Computed partial fraction coefficients for degree 14. Differing
  digits are underlined. \texttt{j} is Python syntax for imaginary numbers
  (e.g., \texttt{2j}$=2i$). Note that~\oldcite{pusa2012correction} only has 19
  digits for $\mathrm{re}(\theta_5)$.}
\label{table:pusa-degree-14}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{ r b{1.8in} b{1.8in} }
Coefficient & Values from~\cite{pusa2012correction} & Values computed by Transmutagen \\
\midrule
\input{pusa-table-16.tex}
\bottomrule
\end{tabular}
\caption{Computed partial fraction coefficients for degree 16. Differing
  digits are underlined. \texttt{j} is Python syntax for imaginary numbers
  (e.g., \texttt{2j}$=2i$). Note that $\mathrm{re}(\alpha_5)$ and
  $\mathrm{re}(\alpha_7)$ differ within machine floating point
  precision.\todo{How do indicate that in the table?}}
\label{table:pusa-degree-16}
\end{table}

\begin{figure}[!ht]
\centering
\resizebox{0.9\textwidth}{!}{\input{pusa-differences.pgf}}
\caption{Difference between $\hat{r}_{k,k}$ for our computed values and the
  values from~\oldcite{pusa2012correction} for $k=14,16$.}
\label{fig:pusa-differences}
\end{figure}

From equation~\ref{eq:part-frac}, we can see that
$\lim_{t\to\infty}{\left|\hat{r}_{k,k} - e^{-t}\right|} = \alpha_0$. By the
equioscillation theorem, the error of the approximation equioscillates $2k$
times in $(0, \infty)$, and additionally takes on the same value at
$\infty$.\footnote{Recall from the Remez algorithm above that on the
  translated interval, there are $2k$ points $z_i$, which represent the error
  of the approximation, and $z_{2(k+1)}=\lim_{t\to 1}{D}$. When the
  approximation is translated from $[-1, 1)$ to $[0, \infty)$, the point
  $z_{2(k + 1)}$ represents the ``error at $\infty$''. c.f.\
  Figure~\ref{fig:cram-plot}.} What this means is that the computed value for
$\alpha_0$ should correspond to the maximum absolute error of the
approximation.

This provides us with a method to test the validity of partial fraction
coefficients. If a value of expression $E(t) = \hat{r}_{k,k}(t) - e^{-t}$
\todo{Better variable name than $E$, to avoid confusion with algorithm above?}
exceeds $\alpha_0$ in absolute value, the coefficients are inconsistent.
Conversely, if the critical points of $E$, which can be found numerically via
gradient descent or by numerically solving the symbolic derivative, correspond
to $\alpha_0$, we can have a degree of confidence that the coefficients are
correct.

Using the method outlined in section~\ref{sec:nsolve-bisection} above, we
computed 24 and 27 critical points of $E$ on $[0, 100]$ for degrees 14 and 16,
respectively. These points were then used as the initial points for
\texttt{nsolve}, which internally uses \texttt{mpmath.findroot}'s secant
solver, to solve $\frac{dE}{dt}$ for the partial fraction expressions
generated transmutagen and from the coefficients
from~\cite{pusa2012correction}.

\todo{Perform this analysis and report the
  results.}
